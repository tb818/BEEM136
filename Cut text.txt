Cut text
"Columns (7,8,10,25) have mixed types."
- Columns 8, 10 and 25 will be dropped for the analysis - no issue
- Col 7, Total Value should be an integer - need to understand why this isn't the case
- All volumes are integers

## Understand date ranges
print(min(provider_data_completed_raw["Fin_YR"]), max(provider_data_completed_raw["Fin_YR"]))
print(min(provider_data_completed_raw["FIN_QTR"]), max(provider_data_completed_raw["FIN_QTR"]))

## Determining volume data issues
val_numeric = pd.to_numeric(provider_data_completed_raw["Total Value"], errors="coerce")
val_non_numeric = provider_data_completed_raw["Total Value"][val_numeric.isna()]
print(val_non_numeric.unique())
print(len(val_non_numeric))

Data much cleaner and stripped back to variables of interest

But difficult to understand which lacode corresponds to which local authorities

Solution: use a lookup table (from the ONS) to map codes to names

# Prints missing local authority names and relevant data
print("Unique local authorities not in data:", missing_las["LAD23NM"].to_list())

# Checking balance more generally
all_quarters = sorted(completed_la_totals["year_quarter"].unique())
all_las = sorted(completed_la_totals["lacode"].unique())
should_be_total_rows = (completed_la_totals["lacode"].nunique()) * (completed_la_totals["year_quarter"].nunique())
actual_total_rows = len(completed_la_totals)
if actual_total_rows == should_be_total_rows:
    print("\nPanel is fully balanced.")
else:
    print("\nPanel is *unbalanced*. There should be", should_be_total_rows,"rows, but there are", actual_total_rows, "rows.")

## PROVIDER DATA
# Getting unique codes from each dataset
codes_in_data = set(completed_la_totals["lacode"].unique())
codes_in_lookup = set(la_lookup_filtered["LAD23CD"].unique())

# English/Welsh LA codes not in data
unused_codes = codes_in_lookup - codes_in_data
missing_las = la_lookup_filtered[la_lookup_filtered["LAD23CD"].isin(unused_codes)]
# Balance check against the computed grid
should_be_total_rows = len(grid)
actual_total_rows = len(balanced_panel)
print("Balanced panel rows:",actual_total_rows,"(expected:",should_be_total_rows,")")

print(balanced_panel.head())
completed_la_totals = balanced_panel

# Chronologically sorting the data
completed_la_totals = completed_la_totals.sort_values(["lacode", "year_quarter"])

# 5 random unique LAs
np.random.seed(136)
sample_las = np.random.choice(completed_la_totals["lacode"].unique(), size=5, replace=False)
print("Randomly selected LAs:", sample_las)

# Looping through the random sample and plot
# Nice 'classic' Python for loop method to finish
for lacode in sample_las:
    df_sub = completed_la_totals[completed_la_totals["lacode"] == lacode]
    localauthority = df_sub["localauthority"].iloc[0]
    plt.figure(figsize=(8, 4))
    plt.plot(df_sub["year_quarter"], df_sub["la_total_volume"])
    plt.title("Legal Aid Cases Over Time: " + localauthority)
    plt.xlabel("Quarter")
    plt.ylabel("Total Volume of Completed Cases")
    plt.xticks([])


# ================================== 1(B) Initial Panel Construction ==================================
# Constructs the panel by work completed
# Outputs by LA x quarter: total volume of cases, total value of cases, and total unique providers

## FUNCTIONS
def gen_calendar_qtr(fy: int, fq: int) -> str:
    """
    Convert financial year (ending in March) and quarters fq to calendar year-quarter.
    Mapping:
      fq=1 -> Q2 of fy; fq=2 -> Q3 of fy; fq=3 -> Q4 of fy; fq=4 -> Q1 of fy+1
    """
    cal_year = fy + 1 if fq == 4 else fy                 # If fq=4, increment year - due to crossing into next calendar year
    cal_qtr = {1: 2, 2: 3, 3: 4, 4: 1}[int(fq)]          # Map financial quarter to calendar quarter (Dictionary mapping - classic Python way)
    return f"{int(cal_year)}-q{int(cal_qtr)}"            # Return formatted string such as "2023-q1", sortable

## INITIAL CLEANING
# Cleaning and selecting relevant columns - using assign from Pandas to perform multiple operations
clean_completed = (provider_data_completed_raw
      # Extracts relevant variables, converting to numeric and applying calendar convert function
      .assign(
          volume=lambda d: d["VOL"],
          value=lambda d: pd.to_numeric(d["Total Value"], errors="coerce"),
          fy_start=lambda d: pd.to_numeric(d["Fin_YR"].astype(str).str[:4]),
          fq=lambda d: d["FIN_QTR"],
          lacode=lambda d: d["LACode"])

      # Building calendar year-quarter label
      .assign(year_quarter=lambda d: d.apply(
            lambda r: gen_calendar_qtr(int(r["fy_start"]), int(r["fq"])), axis=1)))

## COMPUTES
completed_la_totals = (clean_completed
      # Grouping by LA × quarter
      .groupby(["year_quarter","lacode"], as_index=False)
      # Aggregating by LA × quarter - counting value, volume and unique providers by provided firm_code
      .agg(
          la_total_volume=("volume","sum"),
          la_total_value=("value","sum"),
          unique_providers=("firm_code", pd.Series.nunique)))

# National totals per quarter (unique firms across all LAs)
# For use later in analysis
quarterly_totals = (clean_completed
      .groupby("year_quarter", as_index=False)
      .agg(
          total_volume=("volume","sum"),
          total_value=("value","sum"),
          total_unique_providers=("firm_code", pd.Series.nunique)))

# Lookup table of LA codes to names
la_lookup = pd.read_csv(
    raw_files_inputs / "Local_Authority_District_(2022)_to_Local_Authority_District_(2023)_Lookup_for_EW.csv")

# Filtering lookup to include only England (E) and Wales (W) codes
# Note: LAD23CD gives LA code, LAD23NM LA name
la_lookup_filtered = (la_lookup
      .loc[la_lookup["LAD23CD"].str.startswith(("E", "W"))]
      [["LAD23CD", "LAD23NM"]]
      .drop_duplicates(subset="LAD23CD")
      .reset_index(drop=True))

# Building index totals
all_quarters = (
    completed_la_totals["year_quarter"]
      .astype(str)
      .sort_values().unique())

all_las = (
    la_lookup_filtered["LAD23CD"]
      .astype(str)
      .sort_values().unique())

grid = pd.MultiIndex.from_product(
    [all_quarters, all_las],
    names=["year_quarter","lacode"]).to_frame(index=False)

# Creating balanced panel from index totals
balanced_panel = (grid.merge(completed_la_totals, on=["year_quarter","lacode"], how="left"))

# Imputing zero for any missing values
for col in ["la_total_volume","la_total_value","unique_providers"]:
    if col not in balanced_panel:
        balanced_panel[col] = 0
    else:
        balanced_panel[col] = balanced_panel[col].fillna(0)

# Constructing the panel
balanced_panel = (balanced_panel
      .merge(la_lookup_filtered[["LAD23CD", "LAD23NM"]]
      .rename(columns={"LAD23CD": "lacode", "LAD23NM": "localauthority"}),
          on="lacode",
          how="left"))

# Merging quarterly totals
balanced_panel = (balanced_panel.merge(quarterly_totals, on="year_quarter", how="left"))

# TRIMMING DATA TO 2010-Q1 TO 2019-Q4
# Resetting index after trimming
balanced_panel = balanced_panel[
    (balanced_panel["year_quarter"] >= "2010-q1") &
    (balanced_panel["year_quarter"] <= "2019-q4")
].reset_index(drop=True)

Two balancing issues exist:
1. One local authority (Isles of Scilly, pop. 2,300) has no recorded cases in the dataset
2. Not all local authorities have data for every year quarter (the panel is not balanced). This may be caused by the Isle of Scilly not being present, missing data, or may be economically meaningful: legal aid provision may have declined to 0 (and thus no cases recorded) in the time period.

DECISIONS:
1. Impute 0 for all year_quarters for the Isles of Scilly. Justification: most likely reason no data recorded is that no firms base themselves in that LA, as so small
2. Impute 0 for lacode-year_quarter pairs with no data. Justification: most likely reason no data recorded is that no cases funded

If successful, should result in panel balanced with year_quarter*lacode observations.

Notable observations:
1. Huge variance in cases by LA
2. A big drop at a similar point in time. When does this big drop occur? What happened?
3. Sharp rise (almost from 0) at start of dataset. The cause of this is uncertain, and is not reflected in MoJ spending figures on legal aid.

Does LASPO explain these?

Sufficient cause for further investigation. To sense-make and deepen analysis, the following steps will be taken:
1. Data to be trimmed to 2010-q1 to 2019-q4. This captures the 10 year period of austerity under investigation, and removes the possibility of data issues skewing analyses at the start of the period.
2. Linkage to various census variables from 2011.
3. Linkage to an inflation index to make nominal changes comparable.

No errors reported in the construction of the individual panels, and individual checks have yielded expected results. Now to sense check the values of the datasets and construct the final, full_panel to perform checks on duplicated columns/rows.

Finally, to better inform analysis of changes in real spending on legal aid, inflation-adjustment values have been obtained from the ONS' website to 'weight' nominal spending by 2015-q2 values.

The dataset is now complete, cleaned, and ready for analysis. For reproducibility and as a redundancy should this code be unavailable (temporarily or permanently), it has been exported as full_panel.csv.

A number of hurdles were encountered when cleaning and merging the census dataset. First, census data from 2011 is stored piecemeal, requiring me to locate and download 5 files from the ONS' website to obtain all information required.

Second, some of the LA codes recored for the 2011 census ceased to exist in 2023, when the LA provider panel was assembled. Fortunately, it appeared that this was entirely due to the consolidation of smaller LAs into larger, and so a review of the ONS LA boundary changes enabled me to create an LA converter CSV.

Third, some census datasets contained data from Scotland, Northern Ireland, and unidentified LA codes. This required removing ahead of the merge.

Fourth and finally, whilst the census data was rich, it did not contain summary statistics immediately useful for analysis. To address this, I have performed simple row-wise operations to create proportions and grouped totals. These have been sanity checked for their size (i.e. bounded [0,1]) and the entire CSV checked for NA values.

It is important to note that only limited analysis can be performed using census data. Unlike the rest of the panel it is static, capturing only one period in time, near the start of the provider panel data (2011). Whilst this is useful for descriptive analysis (e.g. correlational analysis for which areas lost more legal aid), it limits the possibility for causal inference at later periods, as uncaptured demographic changes may in fact drive results.

This concludes Section 1, cleaning and balancing panel.

Summary statistics calculated to obtain understanding of data for visualisation.

Still positively skewed, but less extremely. Suggests the presence of some local authorities with very high amounts of legal aid expenditure. Returning them:

Appears LAs which are larger or are situated within a city (i.e. Birmingham, Manchester, Sheffield and then Westminster and Camden) have the highest values.

This reflects the highly uneven nature of local authorities by population and wealth, and so is not an outlier per se. However, it reinforces the importance of exploiting cross-sectional form of data by performing within LA comparisons across time rather than between LA comparisons.

Now plotting as standardised indices (for value, volume and val/vol) to better compare evolution over time. Note that 2012-q4 has been chosen as the base quarter (i.e. = 100) as this is the final period LASPO was not in force.

Also plotting the evolution in local authorities (with populations as per 2011 census) without any unique firms providing legal aid services.

The proportion of local authorities as deserts and the population living in them appear to rise approximately proportionally (with appropriate scale adjustments). This implies that the reduction in firms providing legal aid services was spread across local authorities in line with their populations.

To better observe the decline in concentration of firms providing legal aid, the concentration is mapped by linking an ONS shapefile to the data.

To observe the evolution in distribution of value, volume and unique providers, violin plots are used.